{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbTqEUJf_M5x"
      },
      "source": [
        "### Let’s count the number of parameters in GPT-2-XL.\n",
        "- You are only allowed to use the variable names defined below to answer the questions, i.e., `n_layers * n_heads` is allowed, while `4 * d_ffn` is not allowed.\n",
        "- For simplicity, we will **ignore the bias terms** in all questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9AVWuSx_C70"
      },
      "outputs": [],
      "source": [
        "n_layers = 48  # the number of transformer layers (aka. transformer blocks)\n",
        "n_heads = 25   # the number of attention heads in each layer\n",
        "d_model = 1600 # the model dimension\n",
        "d_ffn = 6400   # the FFN (aka. MLP) dimension\n",
        "d_heads = 64   # the attn head dimension\n",
        "n_vocab = 50257 # vocabulary size\n",
        "n_ctx = 1024    # the maximum sequence length the model can process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx_V_P93_Y5n"
      },
      "source": [
        "#### The input embeddings consists of token embeddings and position embeddings. Count the number of parameters in the two embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQeVyMUt_V3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f5a0ae-12fd-47d9-9784-381a5996ed55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80411200\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = n_vocab*d_model\n",
        "print(token_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TneTC1eP_jRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "982479e7-a0d0-48c1-b813-847954f21234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1638400\n"
          ]
        }
      ],
      "source": [
        "position_embeddings = n_ctx*d_model # Hint: n_ctx\n",
        "print(position_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_yaWxaE_fWA"
      },
      "source": [
        "#### Q2.1.2 Multi-Headed Attention consists of W_Q, W_K, W_V, and W_O,\n",
        "- MultiHead(Q, K, V) = Concat(head_1, ..., head_n) W_O\n",
        "- see more details in https://arxiv.org/pdf/1706.03762\n",
        "#### Count the number of parameters in them.\n",
        "- Here we * n_layers to calculate the total number of parameters across layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IO_JpGDB_ebx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cd497c9-3ec1-4b10-9fd0-4db8aade2d47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "122880000\n"
          ]
        }
      ],
      "source": [
        "attn_q = n_layers * n_heads * d_model * d_heads\n",
        "print(attn_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcHxS114__nL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db50bb40-a468-4dfe-e043-01c8c47b2f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "122880000\n"
          ]
        }
      ],
      "source": [
        "attn_k = n_layers * n_heads * d_model * d_heads\n",
        "print(attn_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYXz2_jBADJB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09128060-0ff7-4fc6-cde1-f8f237ebd61d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "122880000\n"
          ]
        }
      ],
      "source": [
        "attn_v = n_layers * n_heads * d_model * d_heads\n",
        "print(attn_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y45J1CNnAIJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67850a98-fa01-4fa0-cfe3-df4b40fcc1bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "122880000\n"
          ]
        }
      ],
      "source": [
        "attn_o = n_layers * n_heads * d_heads * d_model\n",
        "print(attn_o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUT0PRLgA2qE"
      },
      "source": [
        "####The feed-forward network (FFN) in each transformer block consists of two layers, ffn1 and ffn2. Count the number of parameters in them, respectively.\n",
        "- We need to * n_layers to calculate the total number of parameters across layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez8NPTMcAzps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3868914b-2948-4046-a622-ce8211c622b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491520000\n"
          ]
        }
      ],
      "source": [
        "ffn1 = n_layers * d_model * d_ffn\n",
        "print(ffn1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3ISg8S_AwDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7d0cc8f-3907-4875-e2bc-4a8e9a404b4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491520000\n"
          ]
        }
      ],
      "source": [
        "ffn2 = n_layers * d_ffn * d_model\n",
        "print(ffn2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PTf2vMhBNqJ"
      },
      "source": [
        "#### Count the number of parameters in the output embeddings (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auX4xsiwBW-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7fc370b-0e7b-4123-ef3b-05db1e9f4dac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1600\n"
          ]
        }
      ],
      "source": [
        "output_embeddings = d_model\n",
        "print(output_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIUo-IIbCTZA"
      },
      "source": [
        "#### Print out the total number of parameters (1 point).\n",
        "- We do not double-count output_embeddings because GPT-2 shares the weights of token_embeddings and output_embeddings.\n",
        "- For simplicity, we ignore the bias terms and layernorms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31Tn7OxmCZwV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8e0a856-a8cc-45db-a275-044ff88ee3a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.557B\n"
          ]
        }
      ],
      "source": [
        "n_total = token_embeddings + position_embeddings + attn_q + attn_k + attn_v + attn_o + ffn1 + ffn2\n",
        "print(f'{n_total/10**9:.3f}B')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmDac9HhBWDH"
      },
      "source": [
        "#### The majority of parameters are in the FFN layers! Print the percentage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7xNJNoLCajv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "266c5a82-4f96-43cb-d13b-42f8186b2dc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63.2%\n"
          ]
        }
      ],
      "source": [
        "print(f'{(ffn1+ffn2)/n_total:.1%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_cCCCpdDB99"
      },
      "source": [
        "## Understanding different implementations of large language models\n",
        "\n",
        "### Different LLMs use slightly different architectures.\n",
        "- The code is modified from Hugging Face's implementations.\n",
        "- `d_model`: the model dimension\n",
        "- `d_ffn`: the FFN (aka. MLP) dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO82AWr9DRqo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b349d92e-45ea-4de7-9f9c-b56c33aa15cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "class GPT2MLP(nn.Module):\n",
        "    def __init__(self, config, d_model, d_ffn):\n",
        "        super().__init__()\n",
        "        self.fn1 = nn.Linear(d_model, d_ffn)\n",
        "        self.fn2 = nn.Linear(d_ffn, d_model)\n",
        "        self.act = ACT2FN[config.activation_function]\n",
        "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
        "\n",
        "    def forward(self, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.FloatTensor:\n",
        "        hidden_states = self.fn1(hidden_states)\n",
        "        hidden_states = self.act(hidden_states)\n",
        "        hidden_states = self.fn2(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class GPT2Block(nn.Module):\n",
        "    def __init__(self, config, d_model, d_ffn, layer_idx=None):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(d_model, eps=config.layer_norm_epsilon)\n",
        "        self.attn = GPT2_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n",
        "        self.ln_2 = nn.LayerNorm(d_model, eps=config.layer_norm_epsilon)\n",
        "        self.mlp = GPT2MLP(config, d_model, d_ffn)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "    ):\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.ln_1(hidden_states)\n",
        "        attn_output = self.attn(\n",
        "            hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        # residual connection\n",
        "        hidden_states = attn_output + residual\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.ln_2(hidden_states)\n",
        "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
        "        # residual connection\n",
        "        hidden_states = residual + feed_forward_hidden_states\n",
        "\n",
        "        return hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzQFIbWYDT0_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aa37565-26f0-4287-cbaa-f8db9de066c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "class GPTJBlock(nn.Module):\n",
        "    def __init__(self, config, d_model, d_ffn, layer_idx=None):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(d_model, eps=config.layer_norm_epsilon)\n",
        "        self.attn = GPTJ_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)\n",
        "        self.mlp = GPTJMLP(config, d_model, d_ffn)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: Optional[torch.FloatTensor],\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "    ):\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.ln_1(hidden_states)\n",
        "        attn_output = self.attn(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
        "        # residual connection\n",
        "        hidden_states = attn_output + feed_forward_hidden_states + residual\n",
        "\n",
        "        return hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4lRLiVKDWLT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abce685e-260b-41cd-f38b-e36aa9db1160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "class LlamaMLP(nn.Module):\n",
        "    def __init__(self, config, d_model, d_ffn):\n",
        "        super().__init__()\n",
        "        self.gate_proj = nn.Linear(d_model, d_ffn, bias=config.mlp_bias)\n",
        "        self.up_proj = nn.Linear(d_model, d_ffn, bias=config.mlp_bias)\n",
        "        self.down_proj = nn.Linear(d_ffn, d_model, bias=config.mlp_bias)\n",
        "        self.act_fn = ACT2FN[config.hidden_act]\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [BS, d_model]\n",
        "        hidden_states = self.act_fn(self.gate_proj(x)) * self.up_proj(x)\n",
        "        output = self.down_proj(hidden_states)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzDenRPnDYMu"
      },
      "source": [
        "#### What are the differences between `GPTJBlock()` and `GPT2Block()`?\n",
        "- Hint: (1) When do self.attn() and self.mlp() add to the residual stream, respectively? (2) What's the input of self.mlp()?\n",
        "- **Ignore** the differences in bias terms, layernorm, dropout, and activation functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pn_Xy3qDhrf"
      },
      "source": [
        "1 ) Residual Connections and Computational Flow\n",
        "\n",
        "GPTJBlock() : Uses a single residual connection at the end, combining the attention output, MLP output, and original input. This architecture allows for potential parallel computation of attention and MLP layers, which can lead to faster training and inference times. Possibly more challenging gradient flow during training due to the longer path between the input and the final residual connection.\n",
        "\n",
        "GPT2Block() : Uses two separate residual connections, one after the attention layer and another after the MLP layer. This potentially leads to better gradient flow during training, as each major component has its own residual connection, but may cause slower training and inference.\n",
        "\n",
        "2) Input to MLP Layer\n",
        "\n",
        "GPTJBlock(): The MLP receives the same input as the attention layer. This results in:\n",
        " - Potentially faster computation, as the MLP and attention layers can process in parallel.\n",
        " - Possibly better generalization, as the MLP has access to the original input features.\n",
        "\n",
        "GPT2Block(): Input to MLP is output of the Attention layer. This can lead to:\n",
        " - More complex feature interactions, as the MLP operates on attention-enhanced representations.\n",
        " - Potentially slower adaptation to new tasks, as the MLP's input is more specialized.\n",
        "\n",
        "3) Layer Normalization Placement\n",
        "\n",
        "GPTJBlock() : Has only one layer normalization operation at the beginning of the block. This leads to less stable training as compared to GPT2Block as the MLP receives unnormalized inputs.\n",
        "\n",
        "GPT2Block() : Has two layer normalization operations, one before each major component. This leads to more stable training, as each component receives normalized inputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSqczliiD-P6"
      },
      "source": [
        "#### In `LlamaMLP()`, the shape of the input `x` is `[BS, d_model]`. What are the shapes of `self.act_fn(self.gate_proj(x))`, `self.up_proj(x)`, `hidden_states`, and `output`, respectively? Answer with the variable names (4 points).\n",
        "- The type of the activation function (ReLU, SiLU, or Sigmoid) does not change the answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynke0xJJELNo"
      },
      "source": [
        "#### Your answer here\n",
        "- The shape of the output of `self.act_fn(self.gate_proj(x))`: [BS, d_ffn]\n",
        "- The shape of the output of `self.up_proj(x)`: [BS, d_ffn]\n",
        "- The shape of `hidden_states`: [BS, d_ffn]\n",
        "- The shape of `output`: [BS, d_model]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZFhYECBGFuO"
      },
      "source": [
        "# Finetuning\n",
        "\n",
        "Finetune a pretrained language model DistillBERT on sentiment analysis task using IMDb dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3W2F3rR8zmE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ed60ad-819b-45b7-bb7c-0820a981cde5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.15.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-ydErHGG-Fv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c40471c-764f-4819-fa5e-067593477010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#load dataset\n",
        "# we use imdb dataset to finetune the model. We test the model on imdb and sst2.\n",
        "from datasets import load_dataset\n",
        "\n",
        "imdb_dataset = load_dataset('stanfordnlp/imdb')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCdtYhZfIUbn"
      },
      "source": [
        "## 1. Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrVgOCBEITjC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df = imdb_dataset['train']\n",
        "train_x, dev_x, train_y, dev_y = train_test_split(train_df['text'], train_df['label'], test_size=0.1, random_state=42, stratify=train_df['label'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db4fOHuo550S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e47621a-54bb-43d8-b6c3-88d65922315a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2500"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "len(train_x)\n",
        "len(dev_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0-NstVLwf9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "699e0702-81f3-427d-e3a3-40fcf6cb743b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Algie, the Miner\" is one bad and unfunny silent comedy. The timing of the slapstick is completely off. This is the kind of humor with certain sequences that make you wonder if they're supposed to be funny or not. However, the actual quality of the film is irrelevant. This is mandatory viewing for film buffs mainly because its one of the earliest examples of gay cinema. The main character of Algie is an effeminate guy, acting much like the stereotypical \"pansy\" common in many early films. The film has the homophobic attitude common of the time. \"Algie, the Miner\" is pretty awful, but fascinating from a historical viewpoint. (3/10)\n"
          ]
        }
      ],
      "source": [
        "print(train_x[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw7QOg__Lz7t"
      },
      "source": [
        "## 2. Prepare the data\n",
        "\n",
        "\n",
        "We use Dataset class from torch.utils.data to prepare data, and DataLoader class to prepare batches for training.\n",
        "\n",
        "In the SentimentAnalysisDataset class, you need to use DistillBert tokenizer to tokenize the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F7rgxCPO5JO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7393aaa4-2521-492e-8f2f-e89ac6ba682a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAWZHrraLzEb"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class SentimentAnalysisDataset(Dataset):\n",
        "  #write your code here\n",
        "  def __init__(self,data, tokenizer, max_len = None):\n",
        "    self.input = data['input']\n",
        "    self.labels = data['label']\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    self.len = len(self.input)\n",
        "    self.prepare()\n",
        "\n",
        "  def prepare(self):\n",
        "    self.encodings = self.tokenizer(self.input,\n",
        "                                    padding='max_length',\n",
        "                                    truncation=True,\n",
        "                                    max_length=self.max_len,\n",
        "                                    return_tensors='pt')\n",
        "    self.input_ids = self.encodings['input_ids']\n",
        "    self.attention_masks = self.encodings['attention_mask']\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    input_ids = self.input_ids[idx]\n",
        "    attention_mask = self.attention_masks[idx]\n",
        "    label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'labels': label\n",
        "    }\n",
        "\n",
        "# Example of usage\n",
        "# Usage of GPU: due to limit usage of GPU on Colab, we will not train the whole training set, we only run first 20 samples.\n",
        "train = {'input':train_x[:20], 'label':train_y[:20]}\n",
        "# train_df = pd.DataFrame(train).reset_index(drop=True)\n",
        "train_dataset = SentimentAnalysisDataset(train, tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = 4, shuffle = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjmpf2qWQamz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6231138b-b58e-45f8-929d-30968b85fd9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(149)\n"
          ]
        }
      ],
      "source": [
        "print(sum(train_dataset.attention_masks[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeRXj-Hgw-vv"
      },
      "outputs": [],
      "source": [
        "assert sum(train_dataset.attention_masks[0])==149"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRFvGe7p11TO"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "We use DistillBERT as base model. We still need a linear layer to mapping the last hidden state to classes dimension.\n",
        "\n",
        "The model should have a base model, a linear layer, a dropout layer (0.5) and softmax function. The forward function go through all the layers one by one and return the softmax result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUl4nHap1fSX"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bert_model = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "\n",
        "class ClassificationModel(nn.Module):\n",
        "    def __init__(self,base_model,num_classes):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.classifier = nn.Linear(self.base_model.config.hidden_size, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self,input_ids, attention_mask):\n",
        "        if input_ids.dim() == 1:\n",
        "          input_ids = input_ids.unsqueeze(0)\n",
        "          attention_mask = attention_mask.unsqueeze(0)\n",
        "\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        dropout = self.dropout(last_hidden_state)\n",
        "        logits = self.classifier(dropout)\n",
        "\n",
        "        predicts = self.softmax(logits)\n",
        "\n",
        "        return predicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rrqu5dB2wXd"
      },
      "outputs": [],
      "source": [
        "model = ClassificationModel(base_model = bert_model, num_classes = 2 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHjc-17g3Dx4"
      },
      "outputs": [],
      "source": [
        "input_ids = train_dataset.input_ids[0]\n",
        "attention_mask = train_dataset.attention_masks[0]\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  predicts = model(input_ids,attention_mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CilKdHHL7ykO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9755b2-a153-48e4-cfa1-944ef69b2104"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.4507104158401489, 0.5492895841598511]]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "predicts.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mspVXim3Yk3"
      },
      "outputs": [],
      "source": [
        "assert predicts.tolist() == [[0.4507104158401489, 0.5492895841598511]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_Vbj3xMLJr8"
      },
      "source": [
        "## Model finetuning\n",
        "\n",
        "In this section, you need to implement train loop and evaluation loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1_w9rx9Ldqm"
      },
      "source": [
        " Initialize the optimizer. We use AdamW for optimizer and cross entropy loss. (3pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7g8NKCILc0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bea7329-9358-4b10-a8ba-30af92c857c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvCTgYHh-5gL"
      },
      "outputs": [],
      "source": [
        "dev_df = {'input':dev_x[:20], 'label':dev_y[:20]}\n",
        "dev_df_dataset = SentimentAnalysisDataset(dev_df, tokenizer)\n",
        "\n",
        "dev_dataloader = DataLoader(dev_df_dataset, batch_size = 4, shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRnQzQbhOSs8"
      },
      "source": [
        "We use pytorch to implement. For each epoch, we run one training loop and one evaluation loop. At the end of training, we run the model on test set using the best model saved. For one training step, we run forward pass using pretrained model given input. Then we calculate loss and do backward propagation. See the instructions in the code block.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCeuEA4nLawd",
        "outputId": "bd86d1b9-cc3a-49f0-ba15-f0b29c691044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finetuning::  10%|█         | 5/50 [01:11<10:34, 14.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: train loss is 3.528764545917511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finetuning::  20%|██        | 10/50 [02:25<09:49, 14.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train loss is 3.068988800048828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finetuning::  30%|███       | 15/50 [03:33<08:02, 13.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: train loss is 2.912799656391144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finetuning::  40%|████      | 20/50 [04:41<06:48, 13.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: train loss is 3.033766061067581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finetuning::  50%|█████     | 25/50 [05:49<05:43, 13.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: train loss is 2.9004264771938324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finetuning::  60%|██████    | 30/50 [06:56<04:32, 13.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: train loss is 2.732151120901108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finetuning::  70%|███████   | 35/50 [08:05<03:25, 13.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: train loss is 2.578041046857834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finetuning::  80%|████████  | 40/50 [09:14<02:16, 13.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: train loss is 2.224846512079239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finetuning::  90%|█████████ | 45/50 [10:22<01:07, 13.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: train loss is 1.965950846672058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finetuning:: 100%|██████████| 50/50 [11:30<00:00, 13.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: train loss is 1.7839383482933044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFinetuning:: 100%|██████████| 50/50 [11:50<00:00, 14.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: dev loss is 0.7333539843559265\n",
            "The best loss is 0.7333539843559265. Saving checkpoint!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "epochs = 10 # don't change\n",
        "device = \"cpu\"\n",
        "num_training_steps = epochs * len(train_dataloader)\n",
        "best_loss = 10000\n",
        "best_model = None\n",
        "with tqdm(total=num_training_steps, desc='Finetuning:') as pbar:\n",
        "    for epoch in range(epochs):\n",
        "        # training loop\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch in train_dataloader:\n",
        "            '''\n",
        "            Tips:\n",
        "            1. Put the input and model on the same device\n",
        "            2. Use the optimizer correctly\n",
        "            3. Update the train loss. The printed train loss should be train_loss/len(train_dataloader)\n",
        "            '''\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            pbar.update(1)\n",
        "\n",
        "        print(f'Epoch {epoch}: train loss is {train_loss}')\n",
        "    model.eval()\n",
        "    dev_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dev_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            preds = model(input_ids, attention_mask)\n",
        "            loss = loss_fn(preds, labels)\n",
        "            dev_loss += loss.item()\n",
        "\n",
        "        dev_loss = dev_loss / len(dev_dataloader)\n",
        "        print(f'Epoch {epoch}: dev loss is {dev_loss}')\n",
        "        if dev_loss < best_loss:\n",
        "            #save the checkpoint\n",
        "            best_loss = dev_loss\n",
        "            best_model = model.state_dict()\n",
        "            print(f'The best loss is {best_loss}. Saving checkpoint!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBmLJ0vASj9C"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}